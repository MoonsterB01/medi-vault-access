import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.45.0';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
const geminiApiKey = Deno.env.get('GEMINI_API_KEY');

const supabase = createClient(supabaseUrl, supabaseServiceKey);

interface AnalyzeRequest {
  documentId: string;
  fileContent: string;
  contentType: string;
  filename: string;
}

async function extractTextFromContent(fileContent: string, contentType: string, filename: string): Promise<string> {
  console.log(`Extracting text from ${contentType} file: ${filename}`);
  
  try {
    // Handle text-based content directly
    if (contentType.includes('text/')) {
      const decoded = atob(fileContent);
      console.log(`Extracted ${decoded.length} characters from text file`);
      return decoded;
    }
    
    // For PDFs, we'll extract basic text for now
    // In production, you'd use a proper PDF parsing library
    if (contentType.includes('pdf')) {
      console.log('PDF detected - using filename and basic analysis');
      // For now, return filename and document type as searchable content
      // This allows the AI to at least categorize based on filename
      return `PDF Document: ${filename}`;
    }
    
    // For images (could be scanned documents)
    if (contentType.includes('image/')) {
      console.log('Image detected - using filename for analysis');
      return `Image Document: ${filename}`;
    }
    
    console.log(`Unsupported content type: ${contentType}`);
    return `Document: ${filename}`;
  } catch (error) {
    console.error('Error extracting text:', error);
    return `Document: ${filename}`;
  }
}

async function analyzeContentWithGemini(text: string, filename: string): Promise<{
  keywords: string[];
  categories: string[];
  confidence: number;
}> {
  if (!geminiApiKey) {
    throw new Error('Gemini API key not configured');
  }

  const prompt = `Analyze this medical document and extract:
1. Important medical keywords (max 15) - focus on conditions, procedures, medications, body parts, test results
2. Medical categories that best describe this document (max 5) - such as "Lab Results", "Prescription", "Imaging", "Consultation Notes", "Discharge Summary", etc.

Document filename: ${filename}
Content: ${text}

Even if the content is limited (like just a filename), infer the likely medical categories and keywords based on the filename and document type.

Respond ONLY in valid JSON format:
{
  "keywords": ["keyword1", "keyword2", ...],
  "categories": ["category1", "category2", ...],
  "confidence": 0.85
}

Examples of good medical keywords: "blood test", "x-ray", "prescription", "diabetes", "cardiology", "laboratory"
Examples of good categories: "Lab Results", "Prescription", "Imaging", "Consultation Notes"`;

  try {
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${geminiApiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: prompt
          }]
        }],
        generationConfig: {
          temperature: 0.3,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 1024,
        }
      }),
    });

    const data = await response.json();
    
    if (!response.ok) {
      console.error('Gemini API error:', data);
      throw new Error(`Gemini API error: ${data.error?.message || 'Unknown error'}`);
    }

    const generatedText = data.candidates?.[0]?.content?.parts?.[0]?.text;
    if (!generatedText) {
      throw new Error('No content generated by Gemini');
    }

    // Parse the JSON response
    const jsonMatch = generatedText.match(/\{[\s\S]*\}/);
    if (!jsonMatch) {
      throw new Error('Could not parse JSON from Gemini response');
    }

    const analysis = JSON.parse(jsonMatch[0]);
    
    return {
      keywords: Array.isArray(analysis.keywords) ? analysis.keywords.slice(0, 15) : [],
      categories: Array.isArray(analysis.categories) ? analysis.categories.slice(0, 5) : [],
      confidence: typeof analysis.confidence === 'number' ? analysis.confidence : 0.7,
    };

  } catch (error) {
    console.error('Error analyzing content with Gemini:', error);
    
    // Fallback to basic keyword extraction
    const words = text.toLowerCase().split(/\W+/);
    const medicalKeywords = words.filter(word => 
      word.length > 3 && 
      (word.includes('med') || word.includes('lab') || word.includes('test') || 
       word.includes('result') || word.includes('patient') || word.includes('doctor'))
    ).slice(0, 10);
    
    return {
      keywords: medicalKeywords,
      categories: ['General Medical'],
      confidence: 0.3,
    };
  }
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { documentId, fileContent, contentType, filename }: AnalyzeRequest = await req.json();

    console.log(`Analyzing document ${documentId} with type ${contentType}, filename: ${filename}`);

    // Extract text content
    const extractedText = await extractTextFromContent(fileContent, contentType, filename);
    console.log(`Extracted text length: ${extractedText.length}`);
    
    // Analyze content with Gemini
    const analysis = await analyzeContentWithGemini(extractedText, filename);
    console.log(`Analysis completed with ${analysis.keywords.length} keywords and confidence ${analysis.confidence}`);
    
    // Update document with extracted content and analysis
    const { error: updateError } = await supabase
      .from('documents')
      .update({
        extracted_text: extractedText,
        content_keywords: analysis.keywords,
        auto_categories: analysis.categories,
        content_confidence: analysis.confidence,
      })
      .eq('id', documentId);

    if (updateError) {
      throw new Error(`Failed to update document: ${updateError.message}`);
    }

    // Insert keywords into document_keywords table
    if (analysis.keywords.length > 0) {
      const keywordInserts = analysis.keywords.map(keyword => ({
        document_id: documentId,
        keyword,
        keyword_type: 'medical',
        confidence: analysis.confidence,
      }));

      const { error: keywordError } = await supabase
        .from('document_keywords')
        .insert(keywordInserts);

      if (keywordError) {
        console.error('Failed to insert keywords:', keywordError);
      }
    }

    console.log(`Successfully analyzed document ${documentId}`);

    return new Response(JSON.stringify({
      success: true,
      extractedText: extractedText.substring(0, 500), // First 500 chars for preview
      keywords: analysis.keywords,
      categories: analysis.categories,
      confidence: analysis.confidence,
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('Error in analyze-document-content function:', error);
    return new Response(JSON.stringify({ 
      error: error.message,
      success: false 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});